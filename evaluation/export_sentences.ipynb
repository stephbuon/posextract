{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add nchar greater than 40\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def validate_data(df, col):\n",
    "    df = df[df[col].str.match(r'hon\\.$') == False]\n",
    "    df = df[df[col].str.match(r'^(?![A-Z])') == False]\n",
    "    df['num_words'] = df[col].str.split().str.len()\n",
    "    df = df[df['num_words'] > 4]    \n",
    "    return df\n",
    "\n",
    "def syntax_check(doc):\n",
    "    sent_pos = ' '.join([token.pos_ for token in doc])\n",
    "    if 'VERB' in sent_pos and 'NOUN' in sent_pos or 'PRON':\n",
    "        return 'valid'\n",
    "    else:\n",
    "        return 'invalid'\n",
    "\n",
    "def validate_syntax(df):\n",
    "    df['syntax_check'] = df['parsed_text'].apply(syntax_check)\n",
    "    df = df[df['syntax_check'] == 'valid']\n",
    "    del df['syntax_check']\n",
    "    return df\n",
    "\n",
    "def sentence_check(doc):\n",
    "    if re.match(r'^Which (.*)\\?$|^What (.*)\\?$|^Why (.*)\\?$|^Where (.*)\\?$|^When (.*)\\?$', str(doc), re.IGNORECASE):\n",
    "        if doc[1].pos_ == 'NOUN':\n",
    "            return 'interrogative_sent'\n",
    "    #elif re.match(r' and,| but,| or,', str(doc)):\n",
    "    elif str(doc).count(',') > 0:\n",
    "        return 'comp_sent'\n",
    "    elif doc[0].pos_ != 'NOUN':\n",
    "        if doc[0].pos_ != 'PRON':\n",
    "            if doc[1].pos_ == 'VERB': \n",
    "                return 'leftward_sent'\n",
    "\n",
    "def tag_sentence(df):  \n",
    "    df['tag'] = df['parsed_text'].apply(sentence_check)\n",
    "    del df['parsed_text']\n",
    "    return df\n",
    "\n",
    "def collect_sentences(df, col):\n",
    "    df = pd.read_csv(df)\n",
    "    df = df[[col]].copy()\n",
    "        \n",
    "    df = validate_data(df, col)\n",
    "    \n",
    "    df['parsed_text'] = list(nlp.pipe(df[col], disable = ['ent']))\n",
    "    \n",
    "    df = validate_syntax(df)\n",
    "    df = tag_sentence(df)\n",
    "\n",
    "    export_dir = '/users/sbuongiorno/sentence_eval'\n",
    "    if not os.path.exists(export_dir):\n",
    "        os.makedirs(export_dir)\n",
    "        \n",
    "    types = ['leftward_sent', 'interrogative_sent', 'comp_sent']\n",
    "                \n",
    "    for t in types:\n",
    "        out = df[df['tag'] == t]\n",
    "        out = out.sample(1500)\n",
    "        out.to_csv(export_dir + '/' + t + '.csv')\n",
    "            \n",
    "    df = df.sample(n = 1500)\n",
    "    df.to_csv(export_dir + '/' + 'random_sent.csv')\n",
    "    \n",
    "collect_sentences('/scratch/group/pract-txt-mine/hansard_justnine_12192019.csv', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add nchar greater than 40\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def validate_data(df):\n",
    "    df = df[~df['text'].str.match(r'hon\\.$')]\n",
    "    df = df[~df['text'].str.match(r'^(?![A-Z])')]\n",
    "    return df\n",
    "\n",
    "def validate_triples_structure(doc):\n",
    "    for token in doc:\n",
    "        if 'VERB' and 'NOUN' in token.pos_:\n",
    "            return doc\n",
    "\n",
    "def leftward_sent(doc): # extract leftward sentences without wh- movement \n",
    "    if len(doc) > 20:\n",
    "        #return doc if doc[1].pos_ == 'VERB' else 'NaN'\n",
    "        if doc[0].pos_ != 'NOUN' and doc[1].pos_ == 'VERB': \n",
    "            return 'leftward_sent'\n",
    "\n",
    "def interrogative_sent(doc): # extract interrogative sentences (with wh- movement)\n",
    "    if len(doc) > 20:\n",
    "        #return doc if doc.match(r'^Which (.*)\\?$|^What (.*)\\?$', re.IGNORECASE) else 'NaN'\n",
    "        if doc.match(r'^Which (.*)\\?$|^What (.*)\\?$', re.IGNORECASE):\n",
    "            return 'interrogative_sent'\n",
    "\n",
    "def comp_sent(doc): # extract compound, complex, compound-complex sentences\n",
    "    if len(doc) > 40:\n",
    "        #return doc if doc.match(r' and,| but,| or,') else 'NaN'\n",
    "        if doc.match(r' and,| but,| or,'):\n",
    "            return 'comp_sent'\n",
    "\n",
    "def collect_sentences(df, col):\n",
    "    df = pd.read_csv(df)\n",
    "    df = df[df['text']].copy()\n",
    "\n",
    "\n",
    "collect_sentences('~/hansard.csv', 'text')\n",
    "\n",
    "hansard = pd.read_csv('~/hansard.csv')\n",
    "hansard = hansard[hansard['text']].copy()\n",
    "\n",
    "hansard = validate_data(hansard)\n",
    "\n",
    "hansard['text'] = list(nlp.pipe(hansard['text'], disable = ['ent']))\n",
    "\n",
    "hansard['tag'] = hansard['text'].apply(leftward_sent)\n",
    "hansard['tag'] = hansard['text'].apply(interrogative_sent)\n",
    "hansard['tag'] = hansard['text'].apply(comp_sent)\n",
    "\n",
    "types = ['leftward_sent', 'interrogative_sent', 'comp_sent']\n",
    "out = pd.DataFrame()\n",
    "for type in types:\n",
    "    out = hansard[hansard['tag'] == type].sample(1000)\n",
    "    out.to_csv('/users/sbuongiornoz' + type + '.csv')\n",
    "\n",
    "hansard = hansard.sample(n = 1000)\n",
    "hansard.to_csv('~/random_sent.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
