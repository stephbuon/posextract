{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephbuon/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/stephbuon/.local/lib/python3.9/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#add nchar greater than 40\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def validate_data(df):\n",
    "    df = df[~df['text'].str.match(r'hon\\.$')]\n",
    "    df = df[~df['text'].str.match(r'^(?![A-Z])')]\n",
    "    return df\n",
    "\n",
    "def validate_triples_structure(doc):\n",
    "    for token in doc:\n",
    "        if 'VERB' and 'NOUN' in token.pos_:\n",
    "            return doc\n",
    "\n",
    "def leftward_sent(doc): # extract leftward sentences without wh- movement \n",
    "    if len(doc) > 20:\n",
    "        #return doc if doc[1].pos_ == 'VERB' else 'NaN'\n",
    "        if doc[0].pos_ != 'NOUN' and doc[1].pos_ == 'VERB': \n",
    "            return 'leftward_sent'\n",
    "\n",
    "def interrogative_sent(doc): # extract interrogative sentences (with wh- movement)\n",
    "    if len(doc) > 20:\n",
    "        #return doc if doc.match(r'^Which (.*)\\?$|^What (.*)\\?$', re.IGNORECASE) else 'NaN'\n",
    "        if doc.match(r'^Which (.*)\\?$|^What (.*)\\?$', re.IGNORECASE):\n",
    "            return 'interrogative_sent'\n",
    "\n",
    "def comp_sent(doc): # extract compound, complex, compound-complex sentences\n",
    "    if len(doc) > 40:\n",
    "        #return doc if doc.match(r' and,| but,| or,') else 'NaN'\n",
    "        if doc.match(r' and,| but,| or,'):\n",
    "            return 'comp_sent'\n",
    "\n",
    "def export_sentences(import_dir):\n",
    "    df = pd.read_csv(import_dir)\n",
    "\n",
    "\n",
    "\n",
    "export_sentences('~/hansard.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv('~/hansard.csv')\n",
    "hansard = hansard[hansard['text']].copy()\n",
    "\n",
    "hansard = validate_data(hansard)\n",
    "\n",
    "hansard['text'] = list(nlp.pipe(hansard['text'], disable = ['ent']))\n",
    "\n",
    "hansard['tag'] = hansard['text'].apply(leftward_sent)\n",
    "hansard['tag'] = hansard['text'].apply(interrogative_sent)\n",
    "hansard['tag'] = hansard['text'].apply(comp_sent)\n",
    "\n",
    "types = ['leftward_sent', 'interrogative_sent', 'comp_sent']\n",
    "out = pd.DataFrame()\n",
    "for type in types:\n",
    "    out = hansard[hansard['tag'] == type].sample(1000)\n",
    "    out.to_csv('~/' + type + '.csv')\n",
    "\n",
    "hansard = hansard.sample(n = 1000)\n",
    "hansard.to_csv('~/random_sent.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
